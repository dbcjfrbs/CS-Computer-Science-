0. 데이터 인프라 개념 : Source - (추출, 변환) - 저장 - 통계, 예측 분석 - 결과
  - 빅데이터를 다루기 위해서는 데이터가 흐르도록 하는 기반 SW가 필요하다 
  - 다양한 기술들과 오픈소스, SaaS 프로젝트들이 엮여 데이터 인프라를 구축하게 된다.
  - 데이터 기반의 의사결정을 돕기 위한 도구이며 데이터의 도움으로 서비스/제품을 향상시키기 위함이다.

1. 배경
  1) Production system - ERP, CRM, DB : 데이터가 만들어지는 곳이다. 이곳에서 데이터를 분석한다면 시스템별로 분석시스템을 만들어야 한다.
  2) Data Warehouse - 통합된 분석 보고서 작성을 위해 위의 여러 소스들로부터 데이터를 통합하여 저장한다.
     여기서는 1)보다 분석을 위해서 쉽게 데이터를 추출할 수 있는 스키마 구조를 사용한다.
     이를 위한 변환과정이 ETL이다. 데이터 추출, 스키마 구조 변환, 데이터 웨어하우스에 적재한다.
     여기서 문제점은 ET과정이 자동화될 수 없다. 그래서 자동화 될 수 있는 추출, 적재를 수행한 후 변환을 수행한다. -> ELT
     
2. Source : 회사에서 발생하는 데이터 공간 또는 발생하는 원본 소스들을 다른 곳에 보낼 수 있는 곳?
  - OLTP : 은행 계좌 시스템을 처리해주는 데이터베이스로 입금과 출금이 하나의 트랜젝션으로 묶여서 처리된다.
           정규화 데이터, 빠른 처리, 많은 테이블
           데이터가 변경되는 일이 대부분인데 이러한 트랜잭션을 기록하는 데 집중되어 있어서 이 데이터를 다른 곳에 쓰지 못한다.
           이걸 변경된 부분만 뽑아서 다른 DB에 보내주는 게 CDC(Change Data Capture)이다.
           이걸 통해 변경 데이터를 분석 DB로 넘겨서 처리할 수 있게 된다.
  - OLAP : 비정규화 데이터, 느린 쿼리, 적은 테이블
    
  - Application / ERP(전사적 자원 관리) / CRM(고객 관련 체인) : 회사 측면에서 생성되는 데이터
  
  - Event Collector : 유저 측면에서 생성되는 데이터를 모아 다양한 분석제품군 서버로 보내진다. ex) 세그먼트(customer data platform), snowflow
                      하나의 api로 전송이 가능해서 편리하다?

  - log : 우리가 운영하는 모든 서버의 로그로 row data로 활용한다.
  
  - 3rd Party API : 외주 서비스 ex) stripe(온라인 결제 서비스)
  
  - File and Object storage

3. 추출 및 변환 : SQL, R, Python 등의 언어를 활용해 row data를 조작한다.
  - connector : source에서 받은 데이터를 데이터레이크, 웨어하우스로 전달한다. 
                ex) fivetran : 데이터 정규화 및 웨어하우스 로드 후 분석이 가능한 스키마 즉 sql 변환까지 해준다.
                    Panply : ETL + 데이터 웨어하우스

  - data modeling : source와 연결되어 있지 않고 datalake, 웨어하우스와 연결되어 있다.
    ex) dbt : 데이터분석가를 위한 분석 엔지니어링 도구다. 분석가에게 개발환경을 준다. 테스트하는 데 활용된다.
              transformation용 SQL 개발 툴이라고 생각하면 된다. 데이터 분석가들이 SW 개발자처럼 일할 수 있게 돕는다.
  
  - workflow manager : connector와 같은 역할을 하는데 이거를 workflow 단위로 관리하면서 진행한다. 
    ex) apache airflow : 테스크를 스케줄링해서 실행, 분산 실행, 동기적 실행(DAG을 통해 구현), 파이썬 코드 사용 / Dagster

  - spark platform : airflow가 task(빅데이터를 분할 분석하는 작업)을 spark에 시킨다. 파이썬 라이브러리, batch query engine으로 묶인다.
    데규모 데이터 처리를 위한 통합 분석 엔진이다. 여러 컴에 나눠 분산처리를 해줄 수 있게 해주는 솔루션이다.
    apache spack core(RDD기반)을 기반으로 SQL, Stream, ML, 그래프 등에 대한 라이브러리를 붙여 각 영역을 해결한다.
    
    ex) databricks : 스파크를 만든 개발자들이 나와서 차린 회사, spark 기반 데이터 분석 플랫폼이다.
                     스파크가 자동차 엔진이면 databricks는 자동차 한 대라고 보면 된다.
                     자신들의 datalake 등을 만들어 하나의 솔루션화 한거다. 그리고 azure에다가 납품했다.
                     마찬가지로 ASW에다가도 올려서 AWS의 여러 서비스들과 연계해 사용한다.
    
    *하둡 vs 스파크 : 스파크가 하둡 맵리듀스보다 100배 빠르다. 하둡에서 HW에 쿼리 날려 읽어오고 하는 일을 Spark는 RAM에서 작동한다.
                      RDD기반이기 때문이다.
    
    -- 파이썬 라이브러리(, boto, dask, ray)
      1) pandas - 데이터 분석 및 조작 도구 / csv, sql, xls 등의 다양한 확장자 파일을 읽어 조작하고 변환해서 외부 파일로 내보낸다.
                  시각화도 가능하다. sql로 하던 걸 대부분 할 수 있다. 
      2) boto - 파이썬으로 아마존 웹 서비스 EC2, S3같은 클라우드 서비스에 접근하게 해주는 라이브러리다.
      3) dask - 파이썬 코드를 여러 대의 서버에서 병렬적으로 처리할 수 있게 해주는 도구다. 

    -- Batch query engine(hive) : 하둡 위에 올라가 있는 데이터 쿼리하기 위한 엔진
      - 하둡에서 데이터를 쿼리하려면 맵리듀스를 짜야하는데 hive는 sql을 작성하면 이게 맵리듀스로 변환되서 하둡에 있는 데이터를 가져와 처리한다.
      - 스파크에서 hive를 사용도 한다.
   
  - streaming : 하둡처럼 데이터가 웨어하우스 로드되는 시점에서 분석하는 것이 아닌 추출시점에서 데이터에 접근, 처리가 실시간으로 가능하다.
    스파크도 streaming을 지원한다. 하지만 원래 micro batch로서 처리하기 때문에 실시간처럼 보이는 거다. native한 streaming은 아니다.
    
    - Kafka
      - 데이터를 전송하는 source(서비스 및 DB)와 데이터를 받는 target(데이터웨어하우스)의 각 개수가 증가하면서 연결라인이 매우 복잡해졌다.
      - Kafka는 이 중간에서 연결자 역할을 한다. source는 이곳에 데이터를 보내고 target은 이곳으로부터 데이터를 받아오면 된다.
      - source에서 데이터 포맷에 상관없이 kafka에 전송할 수 있다.
      - kafka는 큐 구조이다?
      - 클러스터 내부에 여러 서버를 두어 분산해서 중복해 가지고 있다.
  
  
  
*참고
  - ELT -> ETL로의 변환 : 데이터 웨어하우스의 저장공간이 싸진다. 비싸지는 컴퓨팅 리소스를 필요로 하는 Transform을 나중에 한다.
                          EL은 자동화가 되지만 T는 안되서 EL먼저 진행하고 T한다.
                          
  - DAG : 방향성 비 사이클
  - 기본적으로 데이터 인프라에서 데이터를 처리하는 데 파이썬을 활용한다.
  - 맵리듀스 : 분산 처리하고 합친다.
